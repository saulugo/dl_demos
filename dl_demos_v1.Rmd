---
title: "Deep Learning demos con H2O"
author: "Saúl Lugo"
date: "12 de octubre de 2016"
output: html_document
---

#Ejemplo - Comparando Fronteras de Decisión Entre Algoritmos de ML

Para dibujar fronteras de decisión en clasificadores multi-nivel, y para que el ejercicio sea interesante, vamos a generar los datos de prueba como una nube de puntos que va encolando las clases en una nube en forma de espiral. El objetivo de esto es hacer el que ejercicio de clasificación sea difícil ya que la frontera de decisión no va a ser lineal.

Luego con estos datos generados vamos a entrenar varios modelos de Machine Learning para comparar su performance al intentar aprender la frontera de decisión en la nube de puntos en espiral.

##Generando los datos

###Datos de Entrenamiento

```{r espiral_data}
N <- 200 # numero de puntos por clase
K <- 4 # número de clases
X <- data.frame() 
y <- data.frame() 
 
set.seed(1000)
 
for (j in (1:K)){
  r <- seq(0.05,1,length.out = N) # radius
  t <- seq((j-1)*4.7,j*4.7, length.out = N) + rnorm(N, sd = 0.3) # theta
  Xtemp <- data.frame(x =r*sin(t) , y = r*cos(t)) 
  ytemp <- data.frame(matrix(j, N, 1))
  X <- rbind(X, Xtemp)
  y <- rbind(y, ytemp)
}
 
train <- cbind(X,y)
colnames(train) <- c(colnames(X), 'label')
```

###Graficando los datos de entrenamiento

```{r plotting_data}
x_min <- min(X[,1])-0.2; x_max <- max(X[,1])+0.2
y_min <- min(X[,2])-0.2; y_max <- max(X[,2])+0.2
 
plot(train[,-3],pch=19,col=train[,3],cex=0.5, xlim=c(x_min,x_max),ylim=c(y_min,y_max),main="Nube de Puntos en Espiral - Datos de Training")
```

###Datos de Testing

```{r espiral_data}
N <- 100 # numero de puntos por clase
K <- 4 # número de clases
X <- data.frame() 
y <- data.frame() 
 
set.seed(2000) #fijo una semilla distinta a la de training para obtener datos distintos para testing
 
for (j in (1:K)){
  r <- seq(0.05,1,length.out = N) # radius
  t <- seq((j-1)*4.7,j*4.7, length.out = N) + rnorm(N, sd = 0.3) # theta
  Xtemp <- data.frame(x =r*sin(t) , y = r*cos(t)) 
  ytemp <- data.frame(matrix(j, N, 1))
  X <- rbind(X, Xtemp)
  y <- rbind(y, ytemp)
}
 
test <- cbind(X,y)
colnames(test) <- c(colnames(X), 'label')
```

###Graficando los datos

```{r plotting_data}
x_min <- min(X[,1])-0.2; x_max <- max(X[,1])+0.2
y_min <- min(X[,2])-0.2; y_max <- max(X[,2])+0.2
 
plot(test[,-3],pch=19,col=test[,3],cex=0.5, xlim=c(x_min,x_max),ylim=c(y_min,y_max),main="Nube de Puntos en Espiral - Datos de Testing")
```


#Función para graficar la frontera de decisión

```{r decision_boundary}
plot_boundary <- function(name, model, data, g=grid) {
  library(ggplot2)
  data <- as.data.frame(data) 
  pred <- as.data.frame(h2o.predict(model, g))
  n=0.5*(sqrt(nrow(g))-1)
  d <- 1.5
  h <- d*(-n:n)/n
 
  plot(data[,-3],pch=19,col=data[,3],cex=0.5, xlim=c(-d,d),ylim=c(-d,d),main=name)
  
  mypred <- pred[,1]
  z <- array(ifelse(mypred==1,1,ifelse(mypred==2,2,ifelse(mypred==3,3,ifelse(mypred==4,4,0)))),dim=c(2*n+1,2*n+1))
 
  
  contour(h,h,z,col="blue",lwd=2,add=T)
}
```

#Comparando Modelos

Ahora entrenaremos los siguientes modelos para comparar sus resultados:

- Un modelo de **Deep Learning**, con 1000 epoch y los demás parámetros por default.
- **Gradient Boosting Method**, implementado en H2O en la función gbm 
- **Random Forest**, implementado en H2O en la función randomForest
- **Naive Bayes**, implementado en H2O en la función naiveBayes
- **Generalized Linear Modeling**, implementado en H2O en la función glm

```{r models_training}
library(h2o)

h2o.init(nthreads=-1, ip = "localhost", port = 54321)
h2o.removeAll()

train.hex <- as.h2o(train)
summary(train.hex)
train.hex$label <- as.factor(train.hex$label)
summary(train.hex)

setwd("Documentos/r_projects/deep_learning/")
grid <- h2o.importFile(path = normalizePath("data/grid.csv"))

head(grid)
summary(grid)

#Entrenando los 4 modelos:

dl.model.1000e <- h2o.deeplearning(1:2,3,train.hex,epochs = 1000, model_id = "dl_1000e")
gbm.model <- h2o.gbm(1:2,3,train.hex, model_id = "gbm")
rf.model <- h2o.randomForest(1:2,3,train.hex, model_id = "rf")
nb.model <- h2o.naiveBayes(1:2,3,train.hex, model_id = "nb")
glm.model <- h2o.glm(1:2,3,train.hex,family= "multinomial", model_id = "glm")


#Graficando la Frontera de Decisión sobre los Datos de Training

plot.new()
par(mfrow=c(3,3))
plot_boundary("DL Model 1000 epochs", dl.model.1000e, train.hex, grid)
plot_boundary("GBM Model", gbm.model, train.hex, grid)
plot_boundary("RF Model", rf.model, train.hex, grid)
plot_boundary("Naive Bayes Model", nb.model, train.hex, grid)
plot_boundary("GLM Model", glm.model, train.hex, grid)

#Evaluando los modelos con el dataset de Testing

test.hex <- as.h2o(test)
summary(test.hex)
test.hex$label <- as.factor(test.hex$label)
summary(test.hex)

h2o.confusionMatrix(dl.model.1000e, test.hex)
h2o.confusionMatrix(gbm.model,test.hex)
h2o.confusionMatrix(rf.model,test.hex)
h2o.confusionMatrix(nb.model,test.hex)
h2o.confusionMatrix(glm.model,test.hex)

#Graficando la Frontera de Decisión sobre los datos de Testing


par(mfcol=c(1,2))
plot.new()
plot_boundary("DL Training", dl.model.1000e, train.hex, grid)
plot_boundary("DL Training", dl.model.1000e, train.hex, grid)
plot_boundary("DL Testing", dl.model.1000e, test.hex)

plot_boundary("GBM Training", gbm.model, train.hex, grid)
plot_boundary("GBM Testing", gbm.model, test.hex)

plot_boundary("RF Training", rf.model, train.hex, grid)
plot_boundary("RF Testing", rf.model, test.hex)

plot_boundary("NB Training", nb.model, train.hex, grid)
plot_boundary("NB Testing", nb.model, test.hex)

plot_boundary("GLM Training", glm.model, train.hex, grid)
plot_boundary("GLM Testing", glm.model, test.hex)


#Matriz de Confusión
library(caret)

dl.model.1000e.pred <- h2o.predict(dl.model.1000e,newdata = test.hex)
dl.pred <- as.data.frame(dl.model.1000e.pred)
confusionMatrix(dl.pred$predict,test$label)

gbm.model.pred <- h2o.predict(gbm.model,newdata = test.hex)
gbm.pred <- as.data.frame(gbm.model.pred)
confusionMatrix(gbm.pred$predict,test$label) #aprox acuracy 95.25%

rf.model.pred <- h2o.predict(rf.model,newdata = test.hex)
rf.pred <- as.data.frame(rf.model.pred)
confusionMatrix(rf.pred$predict,test$label) #aprox acuracy 96.75%

nb.model.pred <- h2o.predict(nb.model,newdata = test.hex)
nb.pred <- as.data.frame(nb.model.pred)
confusionMatrix(nb.pred$predict,test$label) #aprox acuracy 41.25%

glm.model.pred <- h2o.predict(glm.model,newdata = test.hex)
glm.pred <- as.data.frame(glm.model.pred)
confusionMatrix(glm.pred$predict,test$label) #aprox acuracy 33%

```

###Conclusión

Vemos que el modelo de DL aprendió mejor la frontera de decisión que los demás modelos. Vemos también que los modelos de Naive Bayes y el de Regresión Multinomial son los que tienen mayor dificultad para aprender la frontera, en especial el de Regresión ya que es un modelo lineal y una frontera lineal no funciona en este problema de clasificación.

Por otro lado, al evaluar el acuracy contra el dataset de test, vemos que RF y GBM dan precisiones alrededor del 95% mientras que el de DL da cerca del 73%. Esto se debe a que el modelo de DL está sufriendo de overfitting y por lo tanto no está generalizando bien.

Vemos que el peor acuracy lo tienen NB y GLM ya que sus fronteras son lineales o casi-lineales y en este problema de clasificación no funcionan bien.

#Afinando el Modelo de Deep Learning

##Variando el Tiempo de Entrenamiento (epochs)

Veamos como varía el aprendizaje de la frontera de decisión variando la cantidad de epochs del modelo:

```{r dl_epoch}
#Entrenamos cuatro modelos con distintos epochs
dl.1e <- h2o.deeplearning(1:2,3,train.hex, epochs = 1, model_id = "dl_1e")
dl.250e <- h2o.deeplearning(1:2,3,train.hex, epochs = 250, model_id = "dl_250e")
dl.750e <- h2o.deeplearning(1:2,3,train.hex, epochs = 750, model_id = "dl_750e")
dl.1000e <- h2o.deeplearning(1:2,3,train.hex, epochs = 1000, model_id = "dl_1000e")

#Grafiquemos la frontera de decisión para comparar
par(mfrow = c(2,2))

plot_boundary("DL 1 epoch", dl.1e, train.hex)
plot_boundary("DL 250 epoch", dl.250e, train.hex)
plot_boundary("DL 750 epoch", dl.750e, train.hex)
plot_boundary("DL 1000 epoch", dl.1000e, train.hex)

#Ahora utilicemos checkpoints para ver la evolución del entrenamiento

dl.1e <- h2o.deeplearning(1:2,3,train.hex, epochs = 1, model_id = "dl_1e")
dl.250e <- h2o.deeplearning(1:2,3,train.hex, epochs = 250, model_id = "dl_250e",checkpoint = "dl_1e")
dl.750e <- h2o.deeplearning(1:2,3,train.hex, epochs = 750, model_id = "dl_750e",checkpoint = "dl_250e")
dl.1000e <- h2o.deeplearning(1:2,3,train.hex, epochs = 1000, model_id = "dl_1000e",checkpoint = "dl_750e")

#Grafiquemos la frontera de decisión para comparar
plot.new()
par(mfrow = c(2,2))

plot_boundary("DL 1 epoch", dl.1e, train.hex)
plot_boundary("DL 250 epoch", dl.250e, train.hex)
plot_boundary("DL 750 epoch", dl.750e, train.hex)
plot_boundary("DL 1000 epoch", dl.1000e, train.hex)

dl.1000e.pred <- h2o.predict(dl.1000e,newdata = test.hex)
dl.pred <- as.data.frame(dl.1000e.pred)
confusionMatrix(dl.pred$predict,test$label)

#Vemos que el acuracy en testing baja a 64%
```


##Variando la Arquitectura de la Red Neuronal

```{r dl_arq}
#Modifiquemos la estrucura y la profundidad de las capas ocultas

dl.h1 <- h2o.deeplearning(1:2,3,train.hex, epochs = 1000,hidden = c(11,13,17,19),model_id = "dl_h1")
dl.h2 <- h2o.deeplearning(1:2,3,train.hex, epochs = 1000,hidden = c(42,42,42,42),model_id = "dl_h2")
dl.h3 <- h2o.deeplearning(1:2,3,train.hex, epochs = 1000,hidden = c(200,200),model_id = "dl_h3")
dl.h4 <- h2o.deeplearning(1:2,3,train.hex, epochs = 1000,hidden = c(1000),model_id = "dl_h4")

plot.new()
par(mfrow = c(2,2))

plot_boundary("DL h=c(11,13,17,19)", dl.h1, train.hex)
plot_boundary("DL h=c(42,84,84,42)", dl.h2, train.hex)
plot_boundary("DL h=c(200,200)", dl.h3, train.hex)
plot_boundary("DL h=c(1000)", dl.h4, train.hex)

dl.h1.pred <- h2o.predict(dl.h1,newdata = test.hex) 
dl.pred <- as.data.frame(dl.h1.pred)
confusionMatrix(dl.pred$predict,test$label) #aprox acuracy 89.5%

dl.h2.pred <- h2o.predict(dl.h2,newdata = test.hex) 
dl.pred <- as.data.frame(dl.h2.pred)
confusionMatrix(dl.pred$predict,test$label) #aprox acuracy 97%

dl.h3.pred <- h2o.predict(dl.h3,newdata = test.hex) 
dl.pred <- as.data.frame(dl.h3.pred)
confusionMatrix(dl.pred$predict,test$label) #aprox acuracy 77.8%

dl.h4.pred <- h2o.predict(dl.h4,newdata = test.hex) 
dl.pred <- as.data.frame(dl.h4.pred)
confusionMatrix(dl.pred$predict,test$label) #aprox acuracy 37.5%

```

Vemos que darle profundidad a la red mejoró mucho el acuracy. Las dos primeras redes tienen acuracy de 90 y 94%, ambas tienen cuatro capas internas. La tercera red tiene h = c(200,200) que es la configuración por default de H2O, con lo que obtuvimos el mismo 77% de acuracy que ya teníamos de la primera parte del ejercicio.

Por último, es interesante señalar el caso de la cuarta red. Vemos que aunque tienen 1000 neuronas, tienen una sola capa oculta. Del gráfico de froteras vemos que es la que peor aprendió la frontera y luego de la matriz de confusión vemos que es la que tiene la precisión más baja (37.5%).

**En definitiva, la profundidad de la red es un factor determinante en este caso**.

#Variando La Función de Activación

De la sección anterior nos quedamos con el modelo **dl.h2** que fue la arquitectura que resultó con mejor performance. Ahora veamos que pasa si dejamos esa arquitectura pero variamos la función de activación.

```{r dl_activation_func}

dl.Tanh <- h2o.deeplearning(1:2,3,train.hex,
                            activation = "Tanh", 
                            hidden = c(42,84,84,42),
                            epochs = 1000,
                            model_id = "dl_tanh")

dl.Maxout <- h2o.deeplearning(1:2,3,train.hex,
                            activation = "Maxout", 
                            hidden = c(42,84,84,42),
                            epochs = 1000,
                            model_id = "dl_maxout")

dl.rect <- h2o.deeplearning(1:2,3,train.hex,
                            activation = "Rectifier", 
                            hidden = c(42,84,84,42),
                            epochs = 1000,
                            model_id = "dl_rect")

#nótese que al utilizar RectifierWithDropout sin especificar la tasa de dropout, se está utilizando 50% por default
dl.rectdrop <- h2o.deeplearning(1:2,3,train.hex,
                            activation = "RectifierWithDropout", 
                            hidden = c(42,84,84,42),
                            epochs = 1000,
                            model_id = "dl_rectdrop")

plot.new()
par(mfrow = c(2,2))

plot_boundary("DL Act = Tanh", dl.Tanh, train.hex)
plot_boundary("DL Act = Maxout", dl.Maxout, train.hex)
plot_boundary("DL Act = Rectifier", dl.rect, train.hex)
plot_boundary("DL Act = RectifierWithDroput", dl.rectdrop, train.hex)

dl.Tanh.pred <- h2o.predict(dl.Tanh,newdata = test.hex) 
dl.pred <- as.data.frame(dl.Tanh.pred)
confusionMatrix(dl.pred$predict,test$label) #aprox acuracy 96.5%

dl.Maxout.pred <- h2o.predict(dl.Maxout,newdata = test.hex) 
dl.pred <- as.data.frame(dl.Maxout.pred)
confusionMatrix(dl.pred$predict,test$label) #aprox acuracy 98%

dl.rect.pred <- h2o.predict(dl.rect,newdata = test.hex) 
dl.pred <- as.data.frame(dl.rect.pred)
confusionMatrix(dl.pred$predict,test$label) #aprox acuracy 96.5%

dl.rectdrop.pred <- h2o.predict(dl.rectdrop,newdata = test.hex) 
dl.pred <- as.data.frame(dl.rectdrop.pred)
confusionMatrix(dl.pred$predict,test$label) #aprox acuracy 53.25%

```

Vemos que el mejor performance nos está dando con la función Maxout. La Maxout es una generalización de la rectifier, por lo que en muchos casos funciona mejor y hace que el modelo llegue a converger más rápido.

El peor performance lo tenemos cuando mezclamos la función de activación rectifier con un Dropout del 50%. Esto es de esperar, ya que en el 50% de los casos estamos apagando neuronas en las capas ocultas. El dropout es un buen método de regularización, pero 50% de probabilidad de dropout es muy alto, como podemos ver en este ejemplo.

#Utilizando Grid Search para Comparar Modelos

Con la funcionalidad **Grid Search** de H2O, se pueden modificar varios parámetros de la Red Neuronal y entrenar varios modelos con distintas configuraciones y luego comparar los resultados para escoger la mejor configuración.

Por ejemplo, hagamos un grid para comparar las distintas configuraciones que hicimos anteriormente de arquitectura de la red y de función de activación.


```{r grid_seach}
h_opt <- list(c(11,13,17,19), c(42,84,84,42))
act_opt <- c("Tanh", "Maxout", "Rectifier")

hyper_params <- list(hidden = h_opt, activation = act_opt, epochs = 1000)

model_grid <- h2o.grid("deeplearning",
                       hyper_params = hyper_params,
                       x = 1:2,
                       y = 3,
                       training_frame = train.hex,
                       validation_frame = test.hex)
#El modelo con el menor logloss es el mod-4 con h = c(42,84,84,42) y función de activación Maxout
model_grid

#Veamos el MSE de todos los modelos sobre el dataset de test
for(model_id in model_grid@model_ids){
  model <- h2o.getModel(model_id)
  mse <- h2o.mse(model, valid = TRUE)
  print(sprintf("Test set MSE: %f",mse))
}

#Vemos también que el modelo 4 tiene el test MSE menor.
```

Vemos que con la ayuda de la funcionalidad **grid search** de H2O podemos probar rápidamente muchas combinaciones de configuraciones de hiper-parámetros y ver cuál es la configuración con el mejor performance.

Evaluemos la configuración del modelo-4 con una Matriz de Confusión:

```{r conf_matrix}

model <- h2o.getModel(model_id = "Grid_DeepLearning_RTMP_sid_863a_2_model_R_1476450286986_2_model_4")

dl.model.pred <- h2o.predict(model,newdata = test.hex) 
dl.pred <- as.data.frame(dl.model.pred)
confusionMatrix(dl.pred$predict,test$label) #aprox acuracy 98.25%
```

#Conclusión Final
En este ejercicio, comparamos el modelo de H2O de Deep Learning contra: Generalized Linear Modeling, Naive Bayes, Gradient Boosting Method, Random Fores, es decir, comparamos a DL con todos los algoritmos de clasificación que tiene disponible al día de hoy H2O.

En una primera iteracción, vimos que el algoritmo con mejor precisión (96.75%) fue Random Forest, seguido de GBM (95.25%). Deep Learning sólo logró alrededor del 77% en esta primera iteración. Además vimos que tanto NB como GLM no son adecuados en este tipo de problema de clasificación donde la frontera de decisión necesaria no es líneal (ni se puede aproximar bien por una función lineal).

Sin embargo, en sucesivas iteraciones, cambiando la arquitectura de la red utilizada en el algoritmo de Deep Learning, agregando capas y neuronas, es decir, agregando profundidad al modelo, la precisión del algoritmo mejoró mucho. Finalmente combinando esta arquitectura de mayor profundidad con la función de activación Maxout se obtuvo una precisión aprox. del 98%, mejorando la precisión del Random Forest y del GBM.

En conclusión, en problemas de clasificación se puede mejorar mucho el performance de una Red Neuronal "jugando" con su configuración. Estas tareas de mejora y estudio de la configuración óptima son fáciles de implementar y documentar trabajando con H2O.

**Nota final:** por ser algoritmos distribuidos y paralelizados, los algoritmos de H2O no son reproducibles, ya que no se puede fijar una semilla cuando el algoritmo está corriendo en más de 1 Core. Por lo tanto, los acuracy a los que se hace referencia en las conclusiones, pueden variar si se vuelven a ejecutar los algoritmos, sin embargo, las relaciones de comparación entre ellos deberían mantenerse similares.
