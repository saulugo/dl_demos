---
title: "Deep Learning demos con H2O"
author: "Saúl Lugo"
date: "12 de octubre de 2016"
output: html_document
---

#Ejemplo - Comparando Fronteras de Decisión Entre Algoritmos de ML

Para dibujar fronteras de decisión en clasificadores multi-nivel, y para que el ejercicio sea interesante, vamos a generar los datos de prueba como una nube de puntos que va encolando las clases en una nube en forma de espiral. El objetivo de esto es hacer el que ejercicio de clasificación sea difícil ya que la frontera de decisión no va a ser lineal.

Luego con estos datos generados vamos a entrenar varios modelos de Machine Learning para comparar su performance al intentar aprender la frontera de decisión en la nube de puntos en espiral.

##Generando los datos


```{r espiral_data}
N <- 200 # numero de puntos por clase
K <- 4 # número de clases
X <- data.frame() # data matrix (each row = single example)
y <- data.frame() # class labels
 
#set.seed(308)
 
for (j in (1:K)){
  r <- seq(0.05,1,length.out = N) # radius
  t <- seq((j-1)*4.7,j*4.7, length.out = N) + rnorm(N, sd = 0.3) # theta
  Xtemp <- data.frame(x =r*sin(t) , y = r*cos(t)) 
  ytemp <- data.frame(matrix(j, N, 1))
  X <- rbind(X, Xtemp)
  y <- rbind(y, ytemp)
}
 
mydata <- cbind(X,y)
colnames(mydata) <- c(colnames(X), 'label')
```

##Graficando los datos

```{r plotting_data}
x_min <- min(X[,1])-0.2; x_max <- max(X[,1])+0.2
y_min <- min(X[,2])-0.2; y_max <- max(X[,2])+0.2
 
plot(mydata[,-3],pch=19,col=mydata[,3],cex=0.5, xlim=c(x_min,x_max),ylim=c(y_min,y_max),main="Nube de Puntos en Espiral")
```

#Función para graficar la frontera de decisión

```{r decision_boundary}
plot_boundary <- function(name, model, data, g) {
  library(ggplot2)
  data <- as.data.frame(data) 
  pred <- as.data.frame(h2o.predict(model, g))
  n=0.5*(sqrt(nrow(g))-1)
  d <- 1.5
  h <- d*(-n:n)/n
 
  plot(data[,-3],pch=19,col=data[,3],cex=0.5, xlim=c(-d,d),ylim=c(-d,d),main=name)
  
  mypred <- pred[,1]
  z <- array(ifelse(mypred==1,1,ifelse(mypred==2,2,ifelse(mypred==3,3,ifelse(mypred==4,4,0)))),dim=c(2*n+1,2*n+1))
 
  
  contour(h,h,z,col="blue",lwd=2,add=T)
}
```

#Comparando Modelos

Ahora entrenemos un modelo de Deep Learning para ver cómo el modelo separa las clases y comparemos el resultado contra otros tres modelos:

- Un **Gradient Boosting Method**, implementado en H2O en la función gbm 
- Un **Random Forest**, implementado en H2O en la función randomForest
- Un **Naive Bayes**, implementado en H2O en la función naiveBayes

```{r models_training}
library(h2o)

h2o.init(nthreads=-1, max_mem_size="2G", ip = "localhost", port = 54321)
h2o.removeAll()

mydata.hex <- as.h2o(mydata)
summary(mydata.hex)
mydata.hex$label <- as.factor(mydata.hex$label)
summary(mydata.hex)

grid <- h2o.importFile("data/grid.csv")

head(grid)
summary(grid)

#Entrenando los 4 modelos:

dl.model <- h2o.deeplearning(1:2,3,mydata.hex,epochs = 1000)
gbm.model <- h2o.gbm(1:2,3,mydata.hex)
rf.model <- h2o.randomForest(1:2,3,mydata.hex)
nb.model <- h2o.naiveBayes(1:2,3,mydata.hex)

#Graficando la Frontera de Decisión

par(mfrow=c(2,2))
plot_boundary("Deep Learning Model", dl.model, mydata.hex, grid)
plot_boundary("GBM Model", gbm.model, mydata.hex, grid)
plot_boundary("RF Model", rf.model, mydata.hex, grid)
plot_boundary("Naive Bayes Model", nb.model, mydata.hex, grid)



```

